// basic information about the workflow
manifest {
  name = 'prevent-nf'
  description = 'PREVENT: PRotein Engineering by Variational frEe eNergy approximaTion'
  version = '0.0.0'
  homePage = 'https://github.com/stracquadaniolab/prevent-nf.git'
  author = 'Evgenii Lobzaev'
  nextflowVersion = '>=20.07.1'
}

// default configuration
executor {
	name = 'local'
	cpus   = 4
	memory = 8.GB
  queueSize = 5
}

process {
	executor = 'local'
	cpus = 2
	memory = 4.GB
	shell = ['/bin/bash', '-euo', 'pipefail']
}

// Export this variable to prevent local Python libraries
// from conflicting with those in the container
env {
  PYTHONNOUSERSITE = 1
}

// profiles to setup runtimes, executors, and process resources
profiles {

    docker {
        docker.enabled         = true
        docker.userEmulation   = true
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false

        process.container = "ghcr.io/stracquadaniolab/prevent-nf:0.0.0"
    }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false

        process.container = "docker://ghcr.io/stracquadaniolab/prevent-nf:0.0.0"
    }

    slurm {
        process.executor='slurm'
        process.queue='compute'
    }

    apple {
      docker.runOptions = '--platform linux/amd64'
    }    

    ci {
        executor.name = 'local'
        executor.cpus   = 2
        executor.memory = 6.GB
        executor.queueSize = 5

        process.executor = 'local'
        process.cpus = 1
        process.memory = 2.GB
        process.shell = ['/bin/bash', '-euo', 'pipefail']
    }

    // profile containing dummy data to test the workflow
    test {
        params{
                  // global, used in more than one subworkflow (Preprocessing,Training,Prior,Posterior)
                  resultsDir              = "./results/test-run/"     // master directory where all output is stored
                  seed                    = 0                         // seed value used in:

                  // free energy calculation
                  energy.fasta_file           = "./testdata/argB.fasta"
                  energy.pdb_file             = "./testdata/1gs5.pdb"
                  
                  // mutagenesis
                  mutagenesis.os               = "macOS" //else: "linux"
                  mutagenesis.n_mutation_sites = [1]   // how many mutations in a sequence
                  mutagenesis.n_mutants        = 5       // how many mutants to produce
                  mutagenesis.seeds            = [11]  // seeds for random sampling
                  mutagenesis.foldx_runs       = 2       // don't do more than 5 

                  
                  // preprocessing:
                  preprocessing.lmin             = 0                   // minimum sequence length to filter sequences from file lists
                  preprocessing.lmax             = 60                 // maximum sequence length to filter sequences from file lists; was 260
                  preprocessing.val_pct          = 0.1                 // relative size of validation set (here 20%)
                  preprocessing.mmseq_clustering =  "--min-seq-id 0.8" // mmseqs2 clustering options 
                  preprocessing.training_list    = [
                                                        "./testdata/dataset-with-energies-short-sequences.fasta"
                                                    ] // list of FASTA files to be cleaned and used for NN training
                  preprocessing.seed_list        = [
                                                      "./testdata/argB-short.fasta"
                                                   ]                  // list of FASTA files to be cleaned and used as seed molecules (to obtain novel variants)
                  preprocessing.weight_sequences = false               // add weights to sequences

                  // training VAE:
                  training.epochs                      = 100         // number of training epochs
                  training.val_freq                    = 3          // frequency of validation error calculation
                  training.checkpoint_freq             = 50         // frequency of model checkpoints
                  training.batch_size                  = 10         // batch size
                  training.learning_rate               = 0.0001     // learning rate
                  training.L2                          = 0.0        // L2 constant for normalisation
                  training.latent_size                 = 32         // dimensionality of latent distribution (Gaussian or Dirichlet)
                  training.condition_on_energy         = false      // whether we will condition on energy when decoding sequence from z
                  training.weighted_sampling           = false      // whether we will use WeightedRandomSampling for batch sampling
                  training.entry_checkpoint            = "None"     // model checkpoint to start training from
                  training.embedding_size              = 32         // embedding size of AAs (and special tokens)
                  training.dropout_prob                = 0.2        // AAs embedding dropout probability
                  training.masking_prob                = 0.0        // input masking probability
                  training.heads                       = 8
                  training.num_layers_encoder          = 6
                  training.num_layers_decoder          = 4

                  // sampling:
                  sampling.seed                 = 0                                     // random seed for sampling (prior and posterior)
                  sampling.n_samples            = 200                                   // how many samples to request (should be a multiple of 100!)
                  sampling.mini_batch_size      = 100                                   // divide n_samples into X number of mini batches
                  sampling.max_length           = 150                                   // max number of steps in sampling
                  sampling.e_value              = 1000.0                                // e-value cutoff
                  sampling.query_coverage       = 2.0                                   // query coverage cutoff
                  sampling.temperature          = [1.0]                                 // list of temperatures for seeded sampling
                  sampling.argmax_first_element = true                                  // argmax selection of the first element of seeded samples (should force "M" for high temperatures)
                  sampling.mmseq_clustering     = "--cluster-reassign --min-seq-id 0.7" // clustering to get representative samples
                  sampling.pytorch_file         = "None"                                // pytorch file   : relevant for VAE_SAMPLE
                  sampling.pickle_file          = "None"                                // pickle file    : relevant for VAE_SAMPLE
                  sampling.seeds_file           = "None"                                // seeds file     : relevant for VAE_SAMPLE
                  sampling.databases            = ["None"]                              // train/val sets : relevant for VAE_SAMPLE

                  // training GRU (supervised learning)
                  training_gru.epochs                      = 10         // number of training epochs
                  training_gru.val_freq                    = 3          // frequency of validation error calculation
                  training_gru.checkpoint_freq             = 5         // frequency of model checkpoints
                  training_gru.batch_size                  = 10         // batch size
                  training_gru.learning_rate               = 0.0001     // learning rate
                  training_gru.L2                          = 0.0        // L2 constant for normalisation
                  training_gru.hidden_size                 = 32         // dimensionality of h for GRU
                  training_gru.num_layers                  = 1          // number of stacked GRUs
                  training_gru.bidirectional               = false      // uni- or bi-directional GRU
                  training_gru.entry_checkpoint            = "None"     // model checkpoint to start training_gru from
                  training_gru.embedding_size              = 32         // embedding size of AAs (and special tokens)
                  training_gru.dropout_prob                = 0.2        // AAs embedding dropout probability

                  // training TF encoder (supervised learning)
                  training_tf.epochs                      = 10         // number of training epochs
                  training_tf.val_freq                    = 3          // frequency of validation error calculation
                  training_tf.checkpoint_freq             = 5         // frequency of model checkpoints
                  training_tf.batch_size                  = 10         // batch size
                  training_tf.learning_rate               = 0.0001     // learning rate
                  training_tf.L2                          = 0.0        // L2 constant for normalisation
                  training_tf.latent_size                 = 64         // dimensionality of latent space
                  training_tf.weighted_sampling           = false      // whether we will use WeightedRandomSampling for batch sampling
                  training_tf.embedding_size              = 512        // embedding size of AAs (and special tokens), will be split across heads
                  training_tf.heads                       = 8          // number of heads
                  training_tf.num_layers_encoder          = 6          // number of stacked TF encoders
                  training_tf.entry_checkpoint            = "None"     // model checkpoint to start training_tf from
                  training_tf.dropout_prob                = 0.2        // AAs embedding dropout probability

                  // using TF encoder to predict energies for sequences in a FASTA file
                  predicting_tf.pickle_file  = "None"
                  predicting_tf.pytorch_file = "None"
                  predicting_tf.fasta_file   = "None"

                  // optimisation:
                  optimisation.pickle_file   = "./results/test-run/training/best-models/model-input.pickle"
                  optimisation.pytorch_file  = "./results/test-run/training/best-models/best-train-error.pytorch"
                  optimisation.learning_rate = 0.1      // learning rate for SGD
                  optimisation.n_restarts    = 3        // number of restart in a single process
                  optimisation.delta_f_tol   = 0.0  // absolute tolerance for function (energy change)
                  optimisation.max_opt_steps = 10     // maximum allowed number of optimisation steps
                  optimisation.seeds         = [0]      // different initial random seeds (each seed corresponds to an individual process)
                  optimisation.seeds_file    = "None"
                  optimisation.databases     = ["None"]

                  // energy estimate
                  energy_estimate.pickle_file  = "None"
                  energy_estimate.pytorch_file = "None"
                  energy_estimate.fasta_file   = "None"

                  // estimating latent space (get mu/sigma for datasets of interest)
                  latent.pickle_file  = "./results/test-run/training/best-models/model-input.pickle"
                  latent.pytorch_file = "./results/test-run/training/best-models/best-train-error.pytorch"
                  latent.fasta_files  = [
                                          "./results/test-run/preprocessing/train-set.fasta",
                                          "./results/test-run/preprocessing/validation-set.fasta",
                                        ]
            }
        }
}

// configuring log information
report {
    enabled = true
    file = "logs/execution_report.html"
}

// monitor progress on Tower
tower {
  accessToken = 'eyJ0aWQiOiAzMzMzfS5iYjkxYjI5ZTg3ZGY3ZGE2NzhlNjUyNTU1OTI1NzNhMDYxYzdiODRi'
  enabled     = true
} 